{"cells":[{"cell_type":"markdown","metadata":{"id":"3tr1jyxY6PlS"},"source":["## Download and unzip"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You can follow this tutorial for more information - https://www.tutorialspoint.com/google_colab/index.htm\n","# You can also see this video - https://www.youtube.com/watch?v=inN8seMm7UI\n","\n","# Mount your Drive - After doing this step, your Google Drive folders are accessible from Google Colab.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":473,"status":"ok","timestamp":1689181100826,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"BONhSjX4sMAF"},"outputs":[],"source":["import gdown\n","def download_google_file(shader_url, output_name):\n","  id_url = \"https://drive.google.com/uc?id=\" + shader_url.split(\"/\")[5]\n","  gdown.download(id_url, output_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32536,"status":"ok","timestamp":1689181133961,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"8IYEqO-5svPd","outputId":"fb504ef4-202c-4d9b-dc76-4a99c27628f7"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/1tEz2wVQjPp1MjVHZLa-Z3uyVBnwljgGF/view?usp=sharing\", \"VIDEOS.zip\")\n","!unzip VIDEOS.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4030,"status":"ok","timestamp":1689181137988,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"39Avi9EMt1cD","outputId":"45b7bae2-5f9e-4f53-c437-bdd21af640e8"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/123AcAQCldRNE6iKpXuCaVtsaR3uHIOeN/view?usp=sharing\", \"GT.zip\")\n","!unzip GT.zip\n","!mkdir -p GT/TRAINING_SET\n","!mv GT_TRAINING_SET_CL0 GT/TRAINING_SET/0\n","!mv GT_TRAINING_SET_CL1 GT/TRAINING_SET/1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3011,"status":"ok","timestamp":1689181140995,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ygjJrXJYqjOn","outputId":"55cf53b2-2000-418f-9115-c63ae937df16"},"outputs":[],"source":["download_google_file(\"https://drive.google.com/file/d/1rXMCtpus2i2UDdSBD9RwWAxnT0wrrXOk/view?usp=sharing\", \"test_code.zip\")\n","!unzip test_code.zip"]},{"cell_type":"markdown","metadata":{"id":"JT9JzXPm6WLl"},"source":["## Frames extraction from videos"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1689181186032,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"CwSrdj6ivMHz"},"outputs":[],"source":["videos_path = \"TRAINING_SET\"\n","frames_path = \"FRAMES\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689181186488,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"1EjVzy0pEEzk","outputId":"5b3555f4-4359-47f2-8c36-31ccafbdd393"},"outputs":[],"source":["!rm -R FRAMES/TRAINING_SET/"]},{"cell_type":"markdown","metadata":{"id":"FpJD2gh2GaX-"},"source":["We use ffmpeg to faster the frame extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1000745,"status":"ok","timestamp":1689182187228,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"pSfjgF7SvE23","outputId":"af02638b-6a17-434c-e82e-4fed5a064b31"},"outputs":[],"source":["import cv2, os, argparse, glob, PIL, tqdm\n","\n","def extract_frames(video):\n","    # Process the video\n","    ret = True # è True fino a che ci sono frame nel video.\n","    cap = cv2.VideoCapture(video)\n","    f = 0\n","    while ret:\n","        ret, img = cap.read() # I frame vengono presi dalla read function e questo viene salvato in img (un numpy array)\n","        if ret:\n","            f += 1\n","            PIL.Image.fromarray(img).save(os.path.join(frames_path, video, \"{:05d}.jpg\".format(f))) # Con la PIL function salviamo l'array come immagine. Con il formato jpeg non sprechiamo troppo spazio ma perdiamo delle informazioni.\n","    cap.release()\n","\n","#Con il codice sottostante prendiamo i path di tutti i video nelle directory. Poi salviamo tutti i frame nei frame_path che creiamo.\n","\n","# For all the videos\n","file_list = [path for path in glob.glob(os.path.join(videos_path,\"**\"), recursive=True) # glob.glob restituisce tutti i path di una directory. Ma noi siamo interessati solo ai file e quindi li prendiamo con os.path.isfile(path)\n","             if os.path.isfile(path)]\n","#print(file_list)\n","for video in tqdm.tqdm(file_list): # Se ho già caricato i frame di questi video li skippo\n","  if os.path.isdir(os.path.join(frames_path, video)):\n","    continue\n","\n","  os.makedirs(os.path.join(frames_path, video))\n","  #extract_frames(video)    # Invece di chiamare la funzione di prima che è lenta utilizzo questa di sotto che è molto più veloce.\n","  #estre tutti i frame\n","  #os.system(\"ffmpeg -i {} {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n","  #per estrarre : 1 frame per secondo\n","  os.system(\"ffmpeg -i {} -r 1/1 {}/{}/$Frame{}.jpg\".format(video, frames_path, video, \"%05d\"))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CplIL8gd6a8j"},"source":["## Dataset Functions"]},{"cell_type":"markdown","metadata":{"id":"kKpc8TVGFuSn"},"source":["We use strprtf to parse RTF files"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9080,"status":"ok","timestamp":1689182938463,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"ZxOLICtI6iY3","outputId":"38fd7ebf-34e9-4bcb-a6bb-289d5cac5f3e"},"outputs":[],"source":["!pip install striprtf\n","!pip install torchinfo\n","import torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7065,"status":"ok","timestamp":1689182945506,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"WD0VmuLtw2Dm"},"outputs":[],"source":["import os\n","import os.path\n","import numpy as np\n","from PIL import Image\n","from torchvision import transforms\n","import torch\n","from typing import List, Union, Tuple, Any\n","from striprtf.striprtf import rtf_to_text\n","import albumentations\n","\n","\n","# ha le informazioni legate ad  ogni video, i metadati.\n","class VideoRecord(object):\n","    \"\"\"\n","    Helper class for class VideoFrameDataset. This class\n","    represents a video sample's metadata.\n","\n","    Args:\n","        root_datapath: the system path to the root folder of the videos.\n","        row: A list with four or more elements where\n","             1) The first element is the path to the video sample's frames excluding\n","             the root_datapath prefix\n","             2) The  second element is the starting frame id of the video\n","             3) The third element is the inclusive ending frame id of the video\n","             4) The fourth element is the label index.\n","             5) any following elements are labels in the case of multi-label classification\n","    \"\"\"\n","    def __init__(self, row, root_datapath):\n","        self._data = row\n","        self._path = os.path.join(root_datapath, row[0])\n","\n","    @property\n","    def path(self) -> str:\n","        return self._path\n","\n","    @property\n","    def num_frames(self) -> int:\n","        return self.end_frame - self.start_frame + 1  # +1 because end frame is inclusive\n","\n","    @property\n","    def start_frame(self) -> int:\n","        return int(self._data[1])\n","\n","    @property\n","    def end_frame(self) -> int:\n","        return int(self._data[2])\n","\n","    @property\n","    def label(self) -> Union[int, List[int]]:\n","        # just one label_id\n","        if len(self._data) == 4:\n","            return int(self._data[3])\n","        # sample associated with multiple labels\n","        else:\n","            return [int(label_id) for label_id in self._data[3:]]\n","\n","# Il parametro test_mode serve per rendere non aleatoria l'estrazione dei frame dal segmento, ovvero prendere sempre gli stessi frame serve per la validation\n","class VideoFrameDataset(torch.utils.data.Dataset):\n","    r\"\"\"\n","    A highly efficient and adaptable dataset class for videos.\n","    Instead of loading every frame of a video,\n","    loads x RGB frames of a video (sparse temporal sampling) and evenly\n","    chooses those frames from start to end of the video, returning\n","    a list of x PIL images or ``FRAMES x CHANNELS x HEIGHT x WIDTH``\n","    tensors.\n","\n","    More specifically, the frame range [START_FRAME, END_FRAME] is divided into NUM_SEGMENTS\n","    segments and FRAMES_PER_SEGMENT consecutive frames are taken from each segment.\n","\n","    Note:\n","        A demonstration of using this class can be seen\n","        in ``demo.py``\n","        https://github.com/RaivoKoot/Video-Dataset-Loading-Pytorch\n","\n","    Note:\n","        This dataset broadly corresponds to the frame sampling technique\n","        introduced in ``Temporal Segment Networks`` at ECCV2016\n","        https://arxiv.org/abs/1608.00859.\n","\n","    Args:\n","        root_path: The root path in which video folders lie.\n","                   this is ROOT_DATA from the description above.\n","        num_segments: The number of segments the video should\n","                      be divided into to sample frames from.\n","        frames_per_segment: The number of frames that should\n","                            be loaded per segment. For each segment's\n","                            frame-range, a random start index or the\n","                            center is chosen, from which frames_per_segment\n","                            consecutive frames are loaded.\n","        imagefile_template: The image filename template that video frame files\n","                            have inside of their video folders as described above.\n","        transform: Transform pipeline that receives a list of numpy images/frames.\n","        test_mode: If True, frames are taken from the center of each\n","                   segment, instead of a random location in each segment.\n","\n","    \"\"\"\n","    def __init__(self,\n","                 root_path: str,\n","                 num_segments: int = 3,\n","                 frames_per_segment: int = 1,\n","                 imagefile_template: str='{:05d}.jpg',\n","                 transform=None,\n","                 totensor=True,\n","                 test_mode: bool = False):\n","        super(VideoFrameDataset, self).__init__()\n","\n","        self.root_path = root_path\n","        self.num_segments = num_segments\n","        self.frames_per_segment = frames_per_segment\n","        self.imagefile_template = imagefile_template\n","        self.test_mode = test_mode\n","\n","        if transform is None:\n","            self.transform = None\n","        else:\n","            additional_targets = {}\n","            for i in range(self.num_segments * self.frames_per_segment - 1):\n","                additional_targets[\"image%d\" % i] = \"image\"\n","            self.transform = albumentations.Compose([transform],\n","                                                    additional_targets=additional_targets,\n","                                                    p=1)\n","        self.totensor = totensor\n","        self.totensor_transform = ImglistOrdictToTensor()\n","\n","        self._parse_annotationfile()\n","        self._sanity_check_samples()\n","\n","    def _load_image(self, directory: str, idx: int) -> Image.Image:\n","        return np.asarray(Image.open(os.path.join(directory, self.imagefile_template.format(idx))).convert('RGB'))\n","\n","    def _parse_annotationfile(self):\n","        self.video_list = []\n","        for class_name in os.listdir(self.root_path):\n","            for video_name in os.listdir(os.path.join(self.root_path, class_name)):\n","                frames_dir = os.path.join(self.root_path, class_name, video_name)\n","                if os.path.isdir(frames_dir):\n","                    frame_path = os.path.join(class_name, video_name)\n","                    end_frame = len(os.listdir(frames_dir))\n","\n","                    annotation_path = frames_dir\\\n","                        .replace(\"\\\\\", \"/\") \\\n","                        .replace(\"FRAMES/\", \"GT/\") \\\n","                        .replace(\".mp4\", \".rtf\")\n","\n","                    with open(annotation_path, 'r') as file:\n","                        text = rtf_to_text(file.read())\n","                    if len(text):\n","                        label = 1\n","                        start_frame = int(text.split(\",\")[0])\n","                        if start_frame == 0:\n","                          start_frame = 1\n","                    else:\n","                        label = 0\n","                        start_frame = 1\n","\n","                    self.video_list.append(VideoRecord(\n","                        [frame_path, start_frame, end_frame, label],\n","                        self.root_path))\n","\n","    def _sanity_check_samples(self):\n","        for record in self.video_list:\n","            if record.num_frames <= 0 or record.start_frame == record.end_frame:\n","                print(f\"\\nDataset Warning: video {record.path} seems to have zero RGB frames on disk!\\n\")\n","\n","            elif record.num_frames < (self.num_segments * self.frames_per_segment):\n","                print(f\"\\nDataset Warning: video {record.path} has {record.num_frames} frames \"\n","                      f\"but the dataloader is set up to load \"\n","                      f\"(num_segments={self.num_segments})*(frames_per_segment={self.frames_per_segment})\"\n","                      f\"={self.num_segments * self.frames_per_segment} frames. Dataloader will throw an \"\n","                      f\"error when trying to load this video.\\n\")\n","\n","    def _get_start_indices(self, record: VideoRecord) -> 'np.ndarray[int]':\n","        \"\"\"\n","        For each segment, choose a start index from where frames\n","        are to be loaded from.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","        Returns:\n","            List of indices of where the frames of each\n","            segment are to be loaded from.\n","        \"\"\"\n","        # choose start indices that are perfectly evenly spread across the video frames.\n","        if self.test_mode:\n","            distance_between_indices = (record.num_frames - self.frames_per_segment + 1) / float(self.num_segments)\n","\n","            start_indices = np.array([int(distance_between_indices / 2.0 + distance_between_indices * x)\n","                                      for x in range(self.num_segments)])\n","        # randomly sample start indices that are approximately evenly spread across the video frames.\n","        else:\n","            max_valid_start_index = (record.num_frames - self.frames_per_segment + 1) // self.num_segments\n","\n","            start_indices = np.multiply(list(range(self.num_segments)), max_valid_start_index) + \\\n","                      np.random.randint(max_valid_start_index, size=self.num_segments)\n","\n","        return start_indices\n","\n","    def __getitem__(self, idx: int) -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        For video with id idx, loads self.NUM_SEGMENTS * self.FRAMES_PER_SEGMENT\n","        frames from evenly chosen locations across the video.\n","\n","        Args:\n","            idx: Video sample index.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","        record: VideoRecord = self.video_list[idx]\n","\n","        frame_start_indices: 'np.ndarray[int]' = self._get_start_indices(record)\n","\n","        return self._get(record, frame_start_indices)\n","\n","    def _get(self, record: VideoRecord, frame_start_indices: 'np.ndarray[int]') -> Union[\n","        Tuple[List[Image.Image], Union[int, List[int]]],\n","        Tuple['torch.Tensor[num_frames, channels, height, width]', Union[int, List[int]]],\n","        Tuple[Any, Union[int, List[int]]],\n","        ]:\n","        \"\"\"\n","        Loads the frames of a video at the corresponding\n","        indices.\n","\n","        Args:\n","            record: VideoRecord denoting a video sample.\n","            frame_start_indices: Indices from which to load consecutive frames from.\n","        Returns:\n","            A tuple of (video, label). Label is either a single\n","            integer or a list of integers in the case of multiple labels.\n","            Video is either 1) a list of PIL images if no transform is used\n","            2) a batch of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH) in the range [0,1]\n","            if the transform \"ImglistToTensor\" is used\n","            3) or anything else if a custom transform is used.\n","        \"\"\"\n","\n","        frame_start_indices = frame_start_indices + record.start_frame\n","        images = list()\n","\n","        # from each start_index, load self.frames_per_segment\n","        # consecutive frames\n","        for start_index in frame_start_indices:\n","            frame_index = int(start_index)\n","\n","            # load self.frames_per_segment consecutive frames\n","            for _ in range(self.frames_per_segment):\n","                image = self._load_image(record.path, frame_index)\n","                images.append(image)\n","\n","                if frame_index < record.end_frame:\n","                    frame_index += 1\n","\n","        if self.transform is not None:\n","            transform_input = {\"image\": images[0]}\n","            for i, image in enumerate(images[1:]):\n","                transform_input[\"image%d\" % i] = image\n","            images = self.transform(**transform_input)\n","\n","        if self.totensor:\n","            images = self.totensor_transform(images)\n","        return images, record.label\n","\n","    def __len__(self):\n","        return len(self.video_list)\n","\n","\n","class ImglistOrdictToTensor(torch.nn.Module):\n","    \"\"\"\n","    Converts a list or a dict of numpy images to a torch.FloatTensor\n","    of shape (NUM_IMAGES x CHANNELS x HEIGHT x WIDTH).\n","    Can be used as first transform for ``VideoFrameDataset``.\n","    \"\"\"\n","    @staticmethod\n","    def forward(img_list_or_dict):\n","        \"\"\"\n","        Converts each numpy image in a list or a dict to\n","        a torch Tensor and stacks them into a single tensor.\n","\n","        Args:\n","            img_list_or_dict: list or dict of numpy images.\n","        Returns:\n","            tensor of size ``NUM_IMAGES x CHANNELS x HEIGHT x WIDTH``\n","        \"\"\"\n","        if isinstance(img_list_or_dict, list):\n","            return torch.stack([transforms.functional.to_tensor(img)\n","                                for img in img_list_or_dict])\n","        else:\n","            return torch.stack([transforms.functional.to_tensor(img_list_or_dict[k])\n","                                for k in img_list_or_dict.keys()])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1689182945506,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"VC8l71-OIwsv"},"outputs":[],"source":["from torch.utils.data import Subset, DataLoader\n","\n","# Function for the K-fold Cross Validation\n","def cross_val_dataloaders(train_dataset, val_dataset=None, K=10, batch_size=32):\n","  if val_dataset is None:\n","    val_dataset = train_dataset\n","\n","  indexes = torch.randperm(len(train_dataset)) % K\n","\n","  dataloader_params = {\"batch_size\": batch_size, \"num_workers\": 1, \"pin_memory\": True}\n","\n","  train_folds, val_folds = [], []\n","  for k in range(K):\n","\n","      val_fold   = Subset(val_dataset,   (indexes==k).nonzero().squeeze())\n","      train_fold = Subset(train_dataset, (indexes!=k).nonzero().squeeze())\n","\n","      val_fold   = DataLoader(val_fold,   shuffle=False, **dataloader_params)\n","      train_fold = DataLoader(train_fold, shuffle=True,  **dataloader_params)\n","\n","      val_folds.append(val_fold)\n","      train_folds.append(train_fold)\n","  return train_folds, val_folds, indexes"]},{"cell_type":"markdown","metadata":{"id":"ludygBqx7Iln"},"source":["## Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3438,"status":"ok","timestamp":1689182948938,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"kBc-au6X7OU6"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","from tensorboard import notebook\n","\n","def start_tensorboard(log_dir):\n","  writer = SummaryWriter(os.path.join(\"runs\", log_dir))\n","\n","  # run tensorboard in background\n","  ! killall tensorboard\n","  %load_ext tensorboard\n","  %tensorboard --logdir ./runs\n","\n","  notebook.list() # View open TensorBoard instances\n","\n","  return writer"]},{"cell_type":"markdown","metadata":{},"source":["## K-Fold Training Common Functions"]},{"cell_type":"markdown","metadata":{},"source":["The size of X must be changed in order to perform the training. Initially X is a tensor of torch.Size([batch_size, 3, 3, 224, 224]), and I change it to a tensor of torch.Size([3*batch_size, 3, 224, 224]), where the first 3 tensors are part of the first tensor of initial X: therefore they are part of the same initial tensor and therefore they are of the same video. Consequently I also have to change y, but I can't simply replicate it 3 times (ex. y=[1 2 3] -> y=[1 2 3 1 2 3]), but I have to do y=[1 2 3] -> y=[1 1 2 2 3 3].\n","The view works:\n","batch_size=0,second dimension=0 (Concat) batch_size=0,second dimension=1 (Concat) batch_size=0,second dimension=2 (Concat) batch_size=1,second dimension=0 (Concat) batch_size=1,second dimension=1 (Concat) batch_size=1,second dimension=2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.utils import make_grid\n","from tqdm import tqdm\n","\n","def one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, epoch_num, device):\n","  model.train()\n","\n","  i_start = epoch_num * len(train_loader)\n","  for i, (X, y) in tqdm(enumerate(train_loader), desc=\"epoch {} - train \".format(epoch_num)):\n","    \n","    #print(\"X:\", X,\" y:\", y)\n","    (batch_size, frames, channels, width, height) = X.shape\n","    X = X.view(-1,channels, width, height)\n","    y = y.repeat_interleave(frames)\n","    #print(\"X:\", X,\" y:\", y)\n","\n","    if i == 0:\n","      writer.add_image('first_batch', make_grid(X))\n","\n","    X = X.cuda()#.to(device)\n","    y = y.cuda().float()\n","\n","    optimizer.zero_grad()\n","    o = model(X)\n","    o = output_activation(o).squeeze()\n","    #print(\"o=\",o,\"shape o\", o.shape, \"y=\", y, \"shape y\", y.shape)\n","    l = lossFunction(o, y)\n","\n","    l.backward()\n","    optimizer.step()\n","\n","    #print(\"o.detach()\", o.detach())\n","    #print(\"y.detach()\", y.detach())\n","    acc = ((o.detach() > .5) == y.detach()).float().mean()\n","    #acc = (o.detach().argmax(-1) == y.detach()).float().mean()\n","\n","    print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(l.detach().item(), acc))\n","    writer.add_scalar('train/loss', l.detach().item(), i_start+i)\n","    writer.add_scalar('train/acc', acc, i_start+i)\n","\n","  model.eval()\n","  print(\"\\nVALIDATION FASE\\n\")\n","\n","  with torch.no_grad():\n","    val_loss = []\n","    val_corr_pred = []\n","    for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch_num)):\n","      #print(\"X:\", X,\" y:\", y)\n","      (batch_size, frames, channels, width, height) = X.shape\n","      X = X.view(-1,channels, width, height)\n","      y = y.repeat_interleave(frames).float()\n","      #print(\"X:\", X,\" y:\", y)\n","\n","      X = X.cuda()#.to(device)\n","      y = y.cuda()#.to(device)\n","\n","      o = model(X)\n","      o = output_activation(o).squeeze()\n","      val_loss.append(lossFunction(o, y))\n","      #print(\"o=\",o,\" y=\", y)\n","      val_corr_pred.append((o > .5) == y)\n","      #val_corr_pred.append(o.argmax(-1) == y)\n","\n","    val_loss = torch.stack(val_loss).mean().item()\n","    val_accuracy = torch.concatenate(val_corr_pred).float().mean().item()\n","\n","    print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, val_accuracy))\n","    writer.add_scalar('val/loss', val_loss, i_start+i)\n","    writer.add_scalar('val/acc', val_accuracy, i_start+i)\n","  return val_loss, val_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision.utils import make_grid\n","from tqdm import tqdm\n","from firenetV2 import FireNetV2\n","\n","# One epoch of training function for the Firenet model\n","def one_epoch2(model, lossFunction, optimizer, train_loader, val_loader, writer, epoch_num, device):\n","  model.train()\n","\n","  i_start = epoch_num * len(train_loader)\n","  print(\"epoch_num\", epoch_num, \"len(train_loader)\", len(train_loader))\n","  for i, (X, y) in tqdm(enumerate(train_loader), desc=\"epoch {} - train \".format(epoch_num)):\n","    (batch_size, frames, channels, width, height) = X.shape\n","    \n","    X = X.view(-1,channels, width, height)\n","    y = y.repeat_interleave(frames)\n","\n","    if i == 0:\n","      writer.add_image('first_batch', make_grid(X))\n","\n","    X = X.cuda()\n","    y = y.cuda().long()\n","\n","    optimizer.zero_grad()\n","    o = model(X)\n","    o.squeeze()\n","    #print(\"o=\",o,\" y=\", y)\n","    l = lossFunction(o, y)\n","\n","    l.backward()\n","    optimizer.step()\n","\n","    #acc = ((o.detach() > .5) == y.detach()).float().mean()\n","    acc = (o.detach().argmax(-1) == y.detach()).float().mean()\n","    print(\"acc\", acc)\n","\n","    print(\"- batch loss and accuracy : {:.7f}\\t{:.4f}\".format(l.detach().item(), acc))\n","    writer.add_scalar('train/loss', l.detach().item(), i_start+i)\n","    writer.add_scalar('train/acc', acc, i_start+i)\n","    print('train/loss', l.detach().item(), i_start+i)\n","    print('train/acc', acc, i_start+i)\n","\n","  model.eval()\n","  print(\"\\nVALIDATION ... \\n\")\n","  \n","  with torch.no_grad():\n","    val_loss = []\n","    val_corr_pred = []\n","    for X, y in tqdm(val_loader, desc=\"epoch {} - validation\".format(epoch_num)):\n","      #print(\"X:\", X,\" y:\", y)\n","      (batch_size, frames, channels, width, height) = X.shape\n","      X = X.view(-1,channels, width, height)\n","      y = y.repeat_interleave(frames).long()\n","      #print(\"X:\", X,\" y:\", y)\n","\n","      X = X.cuda()\n","      y = y.cuda()\n","      o = model(X)\n","      #o = output_activation(o).squeeze()\n","      o.squeeze()\n","      val_loss.append(lossFunction(o, y))\n","      #print(\"o=\",o,\" y=\", y)\n","      #val_corr_pred.append((o > .5) == y)\n","      val_corr_pred.append(o.argmax(-1) == y)\n","\n","    val_loss = torch.stack(val_loss).mean().item()\n","    val_accuracy = torch.concatenate(val_corr_pred).float().mean().item()\n","\n","    print(\"Validation loss and accuracy : {:.7f}\\t{:.4f}\".format(val_loss, val_accuracy))\n","    writer.add_scalar('val/loss', val_loss, i_start+i)\n","    writer.add_scalar('val/acc', val_accuracy, i_start+i)\n","  return val_loss, val_accuracy"]},{"cell_type":"markdown","metadata":{"id":"gnNjEwPcFnMa"},"source":["## Attempt 1: MobileNetV2"]},{"cell_type":"markdown","metadata":{"id":"HaF9Di63MEmb"},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-JvdE6dMPwd"},"outputs":[],"source":["from models import *\n","\n","model = build_MobileNetV2(1)\n","\n","model = network_parameters_MobileNetV2(model)\n","\n","print(torchinfo.summary(model,\n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{"id":"ZbTNOLqrM8PH"},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{"id":"AIW8CJf7NOIF"},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRaqeNr_MuPL"},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n"]},{"cell_type":"markdown","metadata":{"id":"OTSuCQCDM1uF"},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XpubYOnWNc0M"},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIcUeyvzMz5B"},"outputs":[],"source":["# Create K folds and relative dataloaders\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{"id":"4OnB1tgeOplE"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tBbv7LINQreb"},"outputs":[],"source":["# Define loss and optimizer\n","output_activation=torch.nn.Sigmoid()\n","lossFunction = torch.nn.BCELoss()    # Use this for binary classification\n","#lossFunction = torch.nn.CrossEntropyLoss()  # Use this for multiclass classification\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 1000\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"MobileNetV2_exp7_1000epoch_10fold_3segment_1frampersegment_batchsize32\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":496},"executionInfo":{"elapsed":12797,"status":"error","timestamp":1689180600090,"user":{"displayName":"SALVATORE PAOLINO","userId":"15376030192157007259"},"user_tz":-120},"id":"Ax0HH4NIMqqv","outputId":"8f675aba-9e9b-4086-d075-0c70671e6fc7"},"outputs":[],"source":["# Setup training and save the results\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","  model = build_MobileNetV2(1)\n","  model = network_parameters_MobileNetV2(model)\n","  model.cuda()#.cpu() \n","  \n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = optimizer_settings_MobileNetV2(model, lr, lambda_reg, momentum)\n","  \n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Attempt 2: MobileNetV3"]},{"cell_type":"markdown","metadata":{},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import *\n","\n","model = build_MobileNetV3Small(1)\n","\n","model = network_parameters_MobileNetV3Small(model)\n","\n","print(torchinfo.summary(model,\n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create K folds and relative dataloaders\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define loss and optimizer\n","output_activation=torch.nn.Sigmoid()\n","lossFunction = torch.nn.BCELoss()    # Use this for binary classification\n","#lossFunction = torch.nn.CrossEntropyLoss()  # Use this for multiclass classification\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 1000\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"MobileNetV3Small_exp7_1000epoch_10fold_3segment_1frampersegment_batchsize32\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup training and save the results\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","  model = build_MobileNetV3Small(1)\n","  model = network_parameters_MobileNetV3Small(model)\n","  model.cuda()#.cpu() \n","  \n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = optimizer_settings_MobileNetV3Small(model, lr, lambda_reg, momentum)\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Attempt 3: ResNet50"]},{"cell_type":"markdown","metadata":{},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import *\n","\n","model = build_ResNet50(1)\n","\n","model = network_parameters_ResNet50(model)\n","\n","print(torchinfo.summary(model,\n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create K folds and relative dataloaders\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define loss and optimizer\n","output_activation=torch.nn.Sigmoid()\n","lossFunction = torch.nn.BCELoss()    # Use this for binary classification\n","#lossFunction = torch.nn.CrossEntropyLoss()  # Use this for multiclass classification\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 1000\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"ResNet50_exp7_1000epoch_10fold_3segment_1frampersegment_batchsize32\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup training and save the results\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","  model = build_ResNet50(1)\n","  model = network_parameters_ResNet50(model)\n","  model.cuda()#.cpu()\n","  \n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = optimizer_settings_ResNet50(model, lr, lambda_reg, momentum)\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Attempt 4: FireNetV2"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[64] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=64, width=64, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)"]},{"cell_type":"markdown","metadata":{},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Creazione K fold e relativi dataloader\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["### TRAINING CONFIGURATION ###\n","lossFunction = torch.nn.CrossEntropyLoss()  # Nel caso di output_size del modello > 1\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 400\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"shit\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","### STARTING TENSOBOARD ###\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from firenetV2 import FireNetV2\n","\n","# Setup training and save the results\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","\n","  #Set up the model\n","  model = None\n","  model = FireNetV2()\n","  model.cuda()\n","  # network parameters\n","  for param in model.parameters():\n","    param.requires_grad = True\n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-5)\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e8\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch2(model, lossFunction, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n"]},{"cell_type":"markdown","metadata":{},"source":["## Attempt 5: ResNet18"]},{"cell_type":"markdown","metadata":{},"source":["### Model Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from models import *\n","\n","model = build_ResNet18(1)\n","\n","model = network_parameters_ResNet18(model)\n","\n","print(torchinfo.summary(model,\n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n","        verbose=0,\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n","))"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing of data"]},{"cell_type":"markdown","metadata":{},"source":["Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects. The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preprocessing = albumentations.Sequential([\n","        albumentations.Resize(height=224, width=224, interpolation=1, always_apply=True),\n","        albumentations.Normalize(mean=[0.485, 0.456, 0.406],\n","                                 std=[0.229, 0.224, 0.225],\n","                                 max_pixel_value=255.,\n","                                 always_apply=True),\n","    ])\n","\n","augmentation = albumentations.OneOf([\n","        albumentations.HorizontalFlip(p=1.),\n","        ], p=.5)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Creation datafold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset = VideoFrameDataset(root_path=\"FRAMES/TRAINING_SET/\",\n","                                num_segments=3,\n","                                frames_per_segment=1,\n","                                transform=albumentations.Compose([\n","                                    preprocessing,\n","                                    augmentation],\n","                                    p=1.,\n","                                )\n","                                )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create K folds and relative dataloaders\n","K_cross_val = 10\n","batch_size = 32\n","train_folds, val_folds, indexes = cross_val_dataloaders(dataset, dataset, K_cross_val, batch_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define loss and optimizer\n","output_activation=torch.nn.Sigmoid()\n","lossFunction = torch.nn.BCELoss()    # Use this for binary classification\n","#lossFunction = torch.nn.CrossEntropyLoss()  # Use this for multiclass classification\n","lr=0.001\n","momentum = 0.9\n","lambda_reg = 0\n","\n","epochs = 1000\n","early_stopping_patience = 40\n","\n","# create output directory and logger\n","experiment_name = \"ResNet18_exp17_1000epoch_10fold_3segment_1frampersegment_batchsize32\"\n","\n","dirs = os.listdir()\n","\n","if experiment_name not in dirs:\n","  os.makedirs(experiment_name)\n","\n","writer = start_tensorboard(experiment_name)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setup training and save the results\n","torch.save(indexes, os.path.join(experiment_name, \"cross-val-indexes.pt\"))\n","\n","val_losses = torch.zeros(epochs, K_cross_val)\n","val_accuracies = torch.zeros(epochs, K_cross_val)\n","\n","for k in range(K_cross_val):\n","  model = build_ResNet18(1)\n","  model = network_parameters_ResNet18(model)\n","  model.cuda()#.cpu()\n","  \n","  # dataloader, network, optimizer for each fold\n","  train_loader, val_loader = train_folds[k], val_folds[k]\n","  optimizer = optimizer_settings_ResNet18(model, lr, lambda_reg, momentum)\n","\n","  # early stopping and best model saving\n","  early_stopping_counter = early_stopping_patience\n","  min_val_loss = 1e10\n","\n","  # training and validation\n","  for e in range(epochs):\n","    print(\"FOLD {} - EPOCH {}\".format(k, e))\n","\n","    val_loss, val_accuracy = one_epoch(model, lossFunction, output_activation, optimizer, train_loader, val_loader, writer, e, device)\n","\n","    # store the validation metrics\n","    val_losses[e, k] = val_loss\n","    val_accuracies[e, k] = val_accuracy\n","    torch.save(val_losses, os.path.join(experiment_name,'val_losses.pth'))\n","    torch.save(val_accuracies, os.path.join(experiment_name,'val_accuracies.pth'))\n","\n","    # save the best model and check the early stopping criteria\n","    if val_loss < min_val_loss: # save the best model\n","      min_val_loss = val_loss\n","      early_stopping_counter = early_stopping_patience # reset early stopping counter\n","      torch.save(model.state_dict(), os.path.join(experiment_name,'fold_{}_best_model.pth'.format(k)))\n","      print(\"- saved best model with val_loss =\", val_loss, \"and val_accuracy =\", val_accuracy)\n","\n","    if e>0: # early stopping counter update\n","      if val_losses[e, k] > val_losses[e-1, k]:\n","          early_stopping_counter -= 1 # update early stopping counter\n","      else:\n","          early_stopping_counter = early_stopping_patience # reset early stopping counter\n","    if early_stopping_counter == 0: # early stopping\n","        break\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["3tr1jyxY6PlS","WlmnrZf9ClN8","JT9JzXPm6WLl","ludygBqx7Iln","3PbrkTycOoBb","HaF9Di63MEmb","ZbTNOLqrM8PH","OTSuCQCDM1uF","qAvxCVPDSphi","zPPOA0k-Xe27","L3BXA39vDglH","pECIYFywcY6f","BYQh6dvnDm0U","5ZrciHmQQZJD","ZY3SCDFFtawu"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
